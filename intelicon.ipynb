{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314540a-30c1-4d65-ac3c-c8aadc865f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import speech_recognition as sr\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import threading\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Load the pre-trained BART model for text summarization\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Define paths to images and corresponding names\n",
    "image_paths = {\n",
    "    \"C:/project/photos/Goutham.jpg\": \"GOUTHAM\",\n",
    "    \"C:/project/photos/sumanth.jpg\": \"SUMANTH\",\n",
    "    \"C:/project/photos/vamsi.jpg\": \"VAMSI\"\n",
    "}\n",
    "\n",
    "# Load enrolled faces and corresponding names from the system\n",
    "enrolled_faces = {}\n",
    "for path, name in image_paths.items():\n",
    "    image = face_recognition.load_image_file(path)\n",
    "    encoding = face_recognition.face_encodings(image)[0]  # Assuming only one face per image\n",
    "    enrolled_faces[encoding.tobytes()] = name\n",
    "\n",
    "# Function to recognize faces in an image\n",
    "def recognize_faces(image):\n",
    "    \n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    face_locations = face_recognition.face_locations(rgb_image)\n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(rgb_image, face_locations)\n",
    "    \n",
    "    detected_faces = {}\n",
    "    \n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        \n",
    "        for enrolled_encoding, name in enrolled_faces.items():\n",
    "            \n",
    "            enrolled_encoding_np = np.frombuffer(enrolled_encoding, dtype=np.float64)\n",
    "            \n",
    "            distance = np.linalg.norm(np.subtract(face_encoding, enrolled_encoding_np))\n",
    "            \n",
    "            if distance < 0.6:  \n",
    "                detected_faces[face_location] = name\n",
    "        \n",
    "                break  \n",
    "          \n",
    "    return detected_faces\n",
    "\n",
    "# Function to store presentation details in an Excel file and open it\n",
    "def store_presentation_details_in_excel(details):\n",
    "    # Check if the Excel file already exists or not\n",
    "    try:\n",
    "        df = pd.read_excel('presentation_details.xlsx')\n",
    "    except FileNotFoundError:\n",
    "        # Create a new DataFrame if the file does not exist\n",
    "        df = pd.DataFrame(columns=['Presenter', 'Duration', 'Date', 'Summary'])\n",
    "\n",
    "    # Append new presentation details to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame([details], columns=df.columns)], ignore_index=True)\n",
    "\n",
    "    # Write the DataFrame to the Excel file\n",
    "    try:\n",
    "        df.to_excel('presentation_details.xlsx', index=False)\n",
    "        # Open the Excel file\n",
    "        os.startfile('presentation_details.xlsx')\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied. Make sure the file is closed and you have write permssion\")\n",
    "\n",
    "def calculate_wer(reference_text, hypothesis_text):\n",
    "    # Tokenize reference and hypothesis texts into words\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis_text.lower())\n",
    "\n",
    "    # Create a matrix to store the distances\n",
    "    distance_matrix = nltk.edit_distance(reference_tokens, hypothesis_tokens, transpositions=True)\n",
    "\n",
    "    # Get the WER\n",
    "    wer = float(distance_matrix) / len(reference_tokens)  \n",
    "    return wer\n",
    "    \n",
    "def calculate_accuracy(original_text, summary_text):\n",
    "    # Tokenize texts\n",
    "    nltk.download('punkt')\n",
    "    original_sentences = nltk.sent_tokenize(original_text)\n",
    "    summary_sentences = nltk.sent_tokenize(summary_text)\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(summary_text, original_text)\n",
    "\n",
    "    # Calculate BLEU scores\n",
    "    #bleu_scores = corpus_bleu([[sent] for sent in original_sentences], summary_sentences)\n",
    "\n",
    "    return rouge_scores\n",
    "# Function to record speech and summarize text\n",
    "def record_and_summarize_speech(name):\n",
    "    recognizer = sr.Recognizer()\n",
    "    segments = []\n",
    "\n",
    "    print(f\"Recording speech for {name}...\")\n",
    "\n",
    "    # Continuously capture and recognize speech\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=60)  # Record speech for 60 seconds\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"No speech detected within the timeout period.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"Speech Recognition Result:\", text)\n",
    "        segments.append(text)\n",
    "        summary = summarize_text(' '.join(segments))\n",
    "        print(\"Summary:\", summary)\n",
    "       \n",
    "        rouge_scores = calculate_accuracy(text, summary)\n",
    "        wer=calculate_wer(text,summary)\n",
    "        print(\"ROUGE Scores:\", rouge_scores)\n",
    "        print(\"WORD ERROR RATE:\", wer)\n",
    "        # Store presentation details to a dictionary\n",
    "        presentation_details = {\n",
    "            \"Presenter\": name,\n",
    "            \"Duration\": \"60 seconds\",  # Update with actual duration\n",
    "            \"Date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Summary\": summary\n",
    "        }\n",
    "\n",
    "        # Store presentation details in an Excel file and open it\n",
    "        store_presentation_details_in_excel(presentation_details)\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results; {0}\".format(e))\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=200, early_stopping=True, length_penalty=2.0,no_repeat_ngram_size=3)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Function to handle recording\n",
    "def start_recording():\n",
    "    start_time = time.time()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "             print(\"Failed to capture frame.\")\n",
    "             break\n",
    "\n",
    "        detected_faces = recognize_faces(frame)\n",
    "\n",
    "        if detected_faces:\n",
    "            for face_location, name in detected_faces.items():\n",
    "                record_and_summarize_speech(name)\n",
    "\n",
    "                top, right, bottom, left = face_location\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, name, (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "        else:\n",
    "            if time.time() - start_time > 60:\n",
    "                print(\"No face detected for 60 seconds. Stopping recording.\")\n",
    "                break\n",
    "\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Recording stopped.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start recordingqq\n",
    "def prompt_to_start_recording():\n",
    "    while True:\n",
    "        start_recording_option = input(\"Press 'S' to start recording: \")\n",
    "        if start_recording_option.lower() == 's':\n",
    "            start_recording()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid option. Please press 'S' to start recording.\")\n",
    "\n",
    "# Start recording\n",
    "prompt_to_start_recording()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
